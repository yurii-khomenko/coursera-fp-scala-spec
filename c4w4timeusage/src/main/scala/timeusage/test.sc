import org.apache.spark.{HashPartitioner, SparkConf, SparkContext}

@transient lazy val conf: SparkConf = new SparkConf().setMaster("local").setAppName("StackOverflow")
@transient lazy val sc: SparkContext = new SparkContext(conf)

// Assume that our neighbor list was saved as a Spark objectFile
val links = sc.objectFile[(String, Seq[String])]("links").partitionBy(new HashPartitioner(100))
  .persist()
// Initialize each page's rank to 1.0; since we use mapValues, the resulting RDD // will have the same partitioner as links
var ranks = links.mapValues(v => 1.0)
// Run 10 iterations of PageRank
for (i <- 0 until 10) {
  val contributions = links.join(ranks).flatMap {
    case (pageId, (links, rank)) => links.map(dest => (dest, rank / links.size))
  }
  ranks = contributions.reduceByKey((x, y) => x + y).mapValues(v => 0.15 + 0.85 * v)
}
// Write out the final ranks
ranks.saveAsTextFile("ranks")